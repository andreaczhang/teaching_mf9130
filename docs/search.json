[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lecture Notes for MF9130/9130E",
    "section": "",
    "text": "About\nThis is the lecture notes for the course: Introductory Statistics MF9130/MF9130E at the Faculty of Medicine, University of Oslo.\nThis website is currently under development.\nOfficial course website and material please check here."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nThis is how you cite Knuth (1984)\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "11  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "section1.html",
    "href": "section1.html",
    "title": "Section 1",
    "section": "",
    "text": "This section will lay the foundation."
  },
  {
    "objectID": "section1_descrpt_stat.html",
    "href": "section1_descrpt_stat.html",
    "title": "2  Data and descriptive statistics",
    "section": "",
    "text": "Topics:\nTo be filled in"
  },
  {
    "objectID": "section1_probability.html",
    "href": "section1_probability.html",
    "title": "3  Probability",
    "section": "",
    "text": "Topics:\n\nProbability concepts\nConditional probability\nTotal probability\nBayes’ Rule\nSensitivity, specificity and beyond"
  },
  {
    "objectID": "section1_binom_dist.html",
    "href": "section1_binom_dist.html",
    "title": "4  Binomial distribution",
    "section": "",
    "text": "Topics:\n\nDiscrete probability distribution\nBinomial trials and coefficients\nBinomial distribution"
  },
  {
    "objectID": "section1_normal_dist.html",
    "href": "section1_normal_dist.html",
    "title": "5  Normal distribution",
    "section": "",
    "text": "Topics:\n\nContinuous probability distribution\nNormal distribution\nNormal approximation to binomial distribution"
  },
  {
    "objectID": "section2.html",
    "href": "section2.html",
    "title": "Section 2",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "section2_hypo_test.html",
    "href": "section2_hypo_test.html",
    "title": "6  Hypothesis testing",
    "section": "",
    "text": "Topics:\nTo be filled in"
  },
  {
    "objectID": "section2_proportions.html",
    "href": "section2_proportions.html",
    "title": "7  Proportions",
    "section": "",
    "text": "Topics:\n\nProportions\nTests\nComparingg two proportions\nOdds ratio\nRelative risk"
  },
  {
    "objectID": "section2_tables.html",
    "href": "section2_tables.html",
    "title": "8  Tables, chi-sq, exact tests",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "section2_nonpara.html",
    "href": "section2_nonpara.html",
    "title": "9  Non-parametric methods",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "section2_samplesize.html",
    "href": "section2_samplesize.html",
    "title": "10  Sample size and power",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  },
  {
    "objectID": "section3.html",
    "href": "section3.html",
    "title": "Section 3",
    "section": "",
    "text": "Regression"
  },
  {
    "objectID": "section1_probability.html#concept",
    "href": "section1_probability.html#concept",
    "title": "3  Probability",
    "section": "3.1 Concept",
    "text": "3.1 Concept"
  },
  {
    "objectID": "section1_probability.html#basic-concepts",
    "href": "section1_probability.html#basic-concepts",
    "title": "3  Probability",
    "section": "3.1 Basic concepts",
    "text": "3.1 Basic concepts\nProbability expresses a potential for something to happen.\nIt corresponds to the concept of risk in medicine\nIt is an assessment of uncertainty in a situation or event\n\n3.1.1 Brief history\nBlaise Pascal (17th century): founder of probabiliy theory. Motivated by dice and card games\nAndrey Kolmogorov (1933): formulated the rules\n\n\n3.1.2 Bayesian and frequentist probability\nBayesian definition: Degree of belief that some event will occur\nFrequentis definition: Proportions of times that some event occurred in a large number of similar repeated trials\n\n\n\n\n\n\nExplain\n\n\n\nWhy there would be the two schools, key difference, and why we focus on frequentist\n\n\n\n\n3.1.3 Law of Large Numbers\nFrequency of an event is the proportion of times that the event does occur - interpreted as a probability\nLLN: As an experiment is repeated over and over, the observed frequency approaches the actual probability (true probability). Probability as limit of frequency\n\n\n\n\n\n\nExplain\n\n\n\nCheck the example (Child birth)\nHow frequency is linked to LLN. ‘True probability’\n\n\n\n\n\n\n\n\nExample\n\n\n\nChild birth example\n\n\n\n\n\n\n\n\nExample\n\n\n\nCoin tossing"
  },
  {
    "objectID": "section1_probability.html#probability-calculation",
    "href": "section1_probability.html#probability-calculation",
    "title": "3  Probability",
    "section": "3.2 Probability calculation",
    "text": "3.2 Probability calculation\n\n3.2.1 Events and sample space, stochastic trial\n\n(what is)\nuncertain oucome\n\nAll possible outcomes make up the sample space, where each outcome has a probability of occurrrence\nThe sum of all probabilities in the sample space equals 1\nAn event is a single outcome (or a collection of outcomes??)\n\n\n\n\n\n\nExplain\n\n\n\nThis section need to be better explained\n\n\n\n\n\n\n\n\nExamples\n\n\n\nDice tossing\nChild birth\nDiastolic blood pressure\n\n\n\n\n3.2.2 Venn diagram\nOften used to illustrate events and sample space\n\nA and B are events, S is the sample space\nOperators on events\n\nUnion: \\(A \\cup B\\)\nIntersection: \\(A \\cap B\\)\nCombining operators: \\(A \\cap \\bar{B}\\)\n\n\n\n3.2.3 Calculation rules\nThe probability of event A is denoted by \\(P(A)\\). It is between 0 and 1\nThe probability over the whole sample space equals 1\nComplement rule\n\\[P(A) + P(\\bar{A}) = 1\\]\nAdditive rule\nThe occurrence of at least one of the events A or B is \\[P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\] For disjunct events A, B, \\(P(A \\cap B) = 0\\). Hence \\[P(A \\cup B) = P(A) + P(B)\\]\nMultiplicative rule\nProbability of independent events can be multiplied\n\\[P(A \\cap B) = P(A) \\times P(B)\\] Examples: gender of the child in 2 births, throw of 2 dices\n\n\n\n\n\n\nExamples\n\n\n\nChild birth\n\n\n\n\n\n\n\n\nExamples\n\n\n\nDice tossing"
  },
  {
    "objectID": "section1_probability.html#conditional-probability",
    "href": "section1_probability.html#conditional-probability",
    "title": "3  Probability",
    "section": "3.3 Conditional probability",
    "text": "3.3 Conditional probability\nWhat is the probability of getting an outcome A given that it will at least occur in B? \\(P(A|B)\\)\nRescale the probability of events in B: such that the new sample space has probability 1, then\n\\[\\frac{P(A \\cap B)}{P(B)}\\]\n\n\n\n\n\n\nExercise\n\n\n\nweather forecast\n\n\n\n3.3.1 Stochastic independence\nA and B are independent if\n\\[P(A|B) = P(A)\\] Interpretation: probability of A is the same if we also know B occurs\n\n\n\n\n\n\nExamples\n\n\n\nAppendicitis\nInfluenza\n\n\nCalculations can be simplified if there is stochastic independence\n\n\n\n\n\n\nExamples\n\n\n\nChild birth (revisited)\nDice (revisited)"
  },
  {
    "objectID": "section1_probability.html#total-probability",
    "href": "section1_probability.html#total-probability",
    "title": "3  Probability",
    "section": "3.4 Total probability",
    "text": "3.4 Total probability\n\n\n\n\n\n\nExplain\n\n\n\nMotivation for this section\n\n\n\n\n\n\n\n\nExample\n\n\n\nGender of twins"
  },
  {
    "objectID": "section1_probability.html#bayes-theorem",
    "href": "section1_probability.html#bayes-theorem",
    "title": "3  Probability",
    "section": "3.5 Bayes’ theorem",
    "text": "3.5 Bayes’ theorem"
  },
  {
    "objectID": "section1_probability.html#diagnostic-testing",
    "href": "section1_probability.html#diagnostic-testing",
    "title": "3  Probability",
    "section": "3.5 Diagnostic testing",
    "text": "3.5 Diagnostic testing\n\n3.5.1 Concepts\nSensitivity\nSpecificity\nPositive predictive value\nNegative predictive value\n\n\n\n\n\n\nExample\n\n\n\nMammography\n\n\n\n\n\n\n\n\nExample\n\n\n\nHIV testing\n\n\n\n\n\n\n\n\nExample\n\n\n\nELISA test\n\n\n\n\n3.5.2 Computation of PPV, NPV and prevalence\nApplication of Bayes’ theorem\n\n\n\n\n\n\nExample\n\n\n\nHIV testing (revisited)"
  },
  {
    "objectID": "section1_probability.html#bayesian-statistics",
    "href": "section1_probability.html#bayesian-statistics",
    "title": "3  Probability",
    "section": "3.6 Bayesian statistics",
    "text": "3.6 Bayesian statistics"
  },
  {
    "objectID": "section1_binom_dist.html#random-trial",
    "href": "section1_binom_dist.html#random-trial",
    "title": "4  Binomial distribution",
    "section": "4.1 Random trial",
    "text": "4.1 Random trial"
  },
  {
    "objectID": "section1_binom_dist.html#probability-distribution",
    "href": "section1_binom_dist.html#probability-distribution",
    "title": "4  Binomial distribution",
    "section": "4.1 Probability distribution",
    "text": "4.1 Probability distribution\nRandom (stochastic) trial: we do not know the outcome, but we know the set of possible outcomes\nRandom variable: number linked to the outcome\nProbability distribution: set of probabilities for each of the possible outcomes\n\n4.1.1 Bernoulli trials\nOften a process has two outcomes.\n\nCoin tossing: outcomes are head or tail\nHIV test looks for the presence or absence of antibodies in the blood\n\nTwo outcomes of interest:\n\nthrow a dice, you are only interested in whether you get a 6 or not\n\n\n\n4.1.2 Binomial trials\nA series of random trials that satisfy:\n\nin each trial, record whether event A occurs or not\nthe probability of A, \\(P(A)\\) is the same in each trial, it is denoted by \\(p\\)\nall trials are independent\n\nSuppose you carry out n trials, looking for an event A in each trial. The result of a sequence:\n\\[A, \\bar{A}, A, A, \\bar{A}, ..., A\\] Say that \\(A\\) takes place \\(x\\) times. This means \\(\\bar{A}\\) takes place \\(n-x\\) times.\nWhat is the probability for a certain sequence?\nRecall that probabilities for independent events can be multiplied.\n\\[P(sequence) = p \\times (1-p) \\times p ...\\] For \\(x\\) number of \\(p\\) and \\(n-x\\) number of \\(1-p\\),\n\\[P(sequence) = p^{x} (1-p)^{n-x}\\] The order of the sequence does not matter. The number of occurence matters.\n\n\n4.1.3 Binomial coefficient\nWe want to find the number of ways that \\(x\\) objects can be chosen from a tottal of \\(n\\) objects, regardless of order\nBinomial coefficient: \\(\\binom nx\\)\n\\[\\binom nx = \\frac{n!}{x!(n-x)!}\\] “x factorial”: \\(x! = x \\times (x-1) \\times ... 2 \\times 1\\)\nExample: \\(\\binom 4 3 = \\frac{4\\times 3 \\times 2 \\times 1}{3 \\times 2 \\times 1 \\times 1} = 4\\)"
  },
  {
    "objectID": "section1_binom_dist.html#random-trails-and-counting",
    "href": "section1_binom_dist.html#random-trails-and-counting",
    "title": "4  Binomial distribution",
    "section": "4.1 Random trails and counting",
    "text": "4.1 Random trails and counting\nRandom (stochastic) trial: we do not know the outcome, but we know the set of possible outcomes\nRandom variable: number linked to the outcome\nProbability distribution: set of probabilities for each of the possible outcomes\n\n4.1.1 Bernoulli trials\nOften a process has two outcomes.\n\nCoin tossing: outcomes are head or tail\nHIV test looks for the presence or absence of antibodies in the blood\n\nTwo outcomes of interest:\n\nthrow a dice, you are only interested in whether you get a 6 or not\n\n\n\n4.1.2 Binomial trials\nA series of random trials that satisfy:\n\nin each trial, record whether event A occurs or not\nthe probability of A, \\(P(A)\\) is the same in each trial, it is denoted by \\(p\\)\nall trials are independent\n\nSuppose you carry out n trials, looking for an event A in each trial. The result of a sequence:\n\\[A, \\bar{A}, A, A, \\bar{A}, ..., A\\] Say that \\(A\\) takes place \\(x\\) times. This means \\(\\bar{A}\\) takes place \\(n-x\\) times.\nWhat is the probability for a certain sequence?\nRecall that probabilities for independent events can be multiplied.\n\\[P(sequence) = p \\times (1-p) \\times p ...\\] For \\(x\\) number of \\(p\\) and \\(n-x\\) number of \\(1-p\\),\n\\[P(sequence) = p^{x} (1-p)^{n-x}\\] The order of the sequence does not matter. The number of occurence matters.\n\n\n4.1.3 Binomial coefficient\nWe want to find the number of ways that \\(x\\) objects can be chosen from a tottal of \\(n\\) objects, regardless of order\nBinomial coefficient: \\(\\binom nx\\)\n\\[\\binom nx = \\frac{n!}{x!(n-x)!}\\] “x factorial”: \\(x! = x \\times (x-1) \\times ... 2 \\times 1\\)\nExample: \\(\\binom 4 3 = \\frac{4\\times 3 \\times 2 \\times 1}{3 \\times 2 \\times 1 \\times 1} = 4\\)"
  },
  {
    "objectID": "section1_binom_dist.html#binomial-distribution",
    "href": "section1_binom_dist.html#binomial-distribution",
    "title": "4  Binomial distribution",
    "section": "4.3 Binomial distribution",
    "text": "4.3 Binomial distribution\nThe probability that \\(A\\) occurs exact \\(x\\) times is given by\n\\[P(X = x) = \\binom n x p^{x}(1-p)^{n-x}\\]\ni.e. the number of distributing \\(x\\) events in a sequence of length \\(n\\), times the probability that one particular sequence with \\(x\\) events occurs.\n\n\n\n\n\n\nExplain\n\n\n\nExplain what it means to multiply these two components, and how it is related to frequency\n\n\n\n4.3.1 Examples\n\n\n\n\n\n\nExample, derivation\n\n\n\nBlood type example. Demonstrate counting, and how it leads to binomial distribution with 4 outcomes\n\n\n\n\n\n\n\n\nExample\n\n\n\nFamily with 4 kids, 2 boys. Compare theoretical and real-life percentage\n\n\n\n\n\n\n\n\nExample\n\n\n\nMultiple choice exam\n\n\n\n\n4.3.2 Visualize binomial distribution\n\n\n\n\n\n\nExplain (might put it in another section)\n\n\n\nRevisit the definition of mean variance, and relation to descriptive summary statistics\nDemo computation on a few different parameters"
  },
  {
    "objectID": "section1_probability.html#total-probability-bayes-theorem",
    "href": "section1_probability.html#total-probability-bayes-theorem",
    "title": "3  Probability",
    "section": "3.4 Total probability, Bayes’ theorem",
    "text": "3.4 Total probability, Bayes’ theorem\n\n\n\n\n\n\nExplain\n\n\n\nMotivation for this section\n\n\n\n\n\n\n\n\nExample\n\n\n\nGender of twins"
  },
  {
    "objectID": "section1_binom_dist.html#random-variable-and-distribution",
    "href": "section1_binom_dist.html#random-variable-and-distribution",
    "title": "4  Binomial distribution",
    "section": "4.1 Random variable and distribution",
    "text": "4.1 Random variable and distribution\nRandom (stochastic) trial: we do not know the outcome, but we know the set of possible outcomes\nRandom variable: number linked to the outcome\nProbability distribution: set of probabilities for each of the possible outcomes\nProperties of probability distribution\nsum up to 1\nmean, variance, pmf/pdf"
  },
  {
    "objectID": "section1_binom_dist.html#random-trials-and-counting",
    "href": "section1_binom_dist.html#random-trials-and-counting",
    "title": "4  Binomial distribution",
    "section": "4.2 Random trials and counting",
    "text": "4.2 Random trials and counting\n\n4.2.1 Bernoulli trials\nOften a process has two outcomes.\n\nCoin tossing: outcomes are head or tail\nHIV test looks for the presence or absence of antibodies in the blood\n\nTwo outcomes of interest:\n\nthrow a dice, you are only interested in whether you get a 6 or not\n\n\n\n4.2.2 Binomial trials\nA series of random trials that satisfy:\n\nin each trial, record whether event A occurs or not\nthe probability of A, \\(P(A)\\) is the same in each trial, it is denoted by \\(p\\)\nall trials are independent\n\nSuppose you carry out n trials, looking for an event A in each trial. The result of a sequence:\n\\[A, \\bar{A}, A, A, \\bar{A}, ..., A\\] Say that \\(A\\) takes place \\(x\\) times. This means \\(\\bar{A}\\) takes place \\(n-x\\) times.\nWhat is the probability for a certain sequence?\nRecall that probabilities for independent events can be multiplied.\n\\[P(sequence) = p \\times (1-p) \\times p ...\\] For \\(x\\) number of \\(p\\) and \\(n-x\\) number of \\(1-p\\),\n\\[P(sequence) = p^{x} (1-p)^{n-x}\\] The order of the sequence does not matter. The number of occurence matters.\n\n\n4.2.3 Binomial coefficient\nWe want to find the number of ways that \\(x\\) objects can be chosen from a total of \\(n\\) objects, regardless of order\nBinomial coefficient: \\(\\binom nx\\)\n\\[\\binom nx = \\frac{n!}{x!(n-x)!}\\] “x factorial”: \\(x! = x \\times (x-1) \\times ... 2 \\times 1\\)\nExample: \\(\\binom 4 3 = \\frac{4\\times 3 \\times 2 \\times 1}{3 \\times 2 \\times 1 \\times 1} = 4\\)"
  },
  {
    "objectID": "section1_normal_dist.html#probability-distribution-revisited",
    "href": "section1_normal_dist.html#probability-distribution-revisited",
    "title": "5  Normal distribution",
    "section": "5.1 Probability distribution revisited",
    "text": "5.1 Probability distribution revisited\nProbability mass and density. Compare binomial and a continuous dist visually\n\n\n\n\n\n\nExplain\n\n\n\nRevisit the previous section on discrete variables"
  },
  {
    "objectID": "section1_normal_dist.html#normal-distribution",
    "href": "section1_normal_dist.html#normal-distribution",
    "title": "5  Normal distribution",
    "section": "5.2 Normal distribution",
    "text": "5.2 Normal distribution\nImportant distribution:\n\napplication 1\napplication 2\n\nProbability density function\n\\[f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp(- \\frac{(x-\\mu)^2}{2\\sigma^2})\\]\n\n\\(\\mu\\) is the mean\n\\(\\sigma\\) is the standard deviation\n\\(exp(x) = e^{x}\\)\n\n\n\n5.2.1 Properties\n\nSymmetric, bell-shape\n\\(\\mu\\) and \\(\\sigma\\) define the location and variation\nFrom the center (mean), going two standard deviations each way covers approximately 95% of the distribution\n\n\n\n\n\n\n\nExamples\n\n\n\nSome other parameters\n\n\n\n\n5.2.2 Standard normal distribution\nA normal distribution \\(N(\\mu, \\sigma)\\) with \\(\\mu = 0, \\sigma = 1\\)\nAny normal disribution can be transformed into a standard normal distribution \\(N(0, 1)\\): if \\(X \\sim N(\\mu, \\sigma)\\), then\n\\[Y = \\frac{X-\\mu}{\\sigma} \\sim N(0, 1)\\]\nStandard normal distribution probabilities are commonly presented in tables, so people can check them easily.\n\n\n\n\n\n\nExamples\n\n\n\nbirthweight, compute proportions"
  },
  {
    "objectID": "section1_normal_dist.html#normal-approximation-to-the-binomial-distribution",
    "href": "section1_normal_dist.html#normal-approximation-to-the-binomial-distribution",
    "title": "5  Normal distribution",
    "section": "5.3 Normal approximation to the binomial distribution",
    "text": "5.3 Normal approximation to the binomial distribution\nRecall the binomial distribution,\n\\[P(X = x) = \\binom nx p^x (1-p)^{n-x}\\] where \\(\\binom nx = \\frac{n!}{x!(n-x)!}\\), and \\(\\binom n 0 = 1\\)\nFor large \\(n\\), use the normal distribution.\n\n\n5.3.1 Normal vs binomial\nFit a normal distribution to approximate a binomial distribution\n\n\\(\\mu = np\\)\n\\(\\sigma = \\sqrt{np(1-p)}\\)\n\n\n\n\n\n\n\nExample\n\n\n\nnumber of boys out of 20 new borns\n\n\n\n\n\n5.3.2 Central limit theorem CLT\nCLT in simple words: the distribution of sample mean will be nearly normal regardless of what distribution the variable in the population is, as long as the sample size is large enough.\n\n\n\n\n\n\nExplain\n\n\n\nPoint out that the observed proportion of boys can be considered as a mean\nand the rule of thumb\n\n\n\\[Binom(n, p) \\rightarrow N(\\mu, \\sigma)\\quad \\text{for} \\quad n \\rightarrow \\infty\\]\nif \\(np \\geq 5\\) and \\(n(1-p) \\geq 5\\)"
  },
  {
    "objectID": "section2_proportions.html#proportions",
    "href": "section2_proportions.html#proportions",
    "title": "7  Analysis of proportions",
    "section": "7.1 Proportions",
    "text": "7.1 Proportions\n\n7.1.1 Proportion and binomial distribution\nDeonte two possible outcomes of a binary variable by D (disease) and H (healthy). We want to study the probability, or risk \\(\\pi\\), that D occurs in the population.\nThe sample proportion \\(p\\) is defined as the proportion of individuals in the sample in category D,\n\\[p = \\frac{d}{n}\\] where \\(d\\) is the number of subjects in D, \\(n\\) is the total number in the sample.\nRecall that the sampling distribution of a proportion is the binomial distribution:\n\nindependent experiments\ntwo outcomes: success or not\nprobability of success is the same in all experiments\nprobability of success \\(+\\) probability of no success \\(=1\\)\n\nThe probability of getting exactly \\(d\\) events in a sample of \\(n\\) individuals is\n\\[P(d) = \\binom nd \\pi^{d}(1-\\pi)^{n-d}\\]\nGoal: estimate the true, unknown population risk (probability) \\(\\pi\\) using \\(p\\).\nAlso would like to know the uncertainty of \\(p\\): how far does it differ from \\(\\pi\\)?\n\n\n7.1.2 Uncertainty of a proportion\n\n\n\n\n\n\nexplain\n\n\n\ncheck the def of se, distinguish from sd;\nalso double check how to explain the estimation using sample proportion\nmake sure to explain when and where \\(\\pi, p\\) are appropriate\n\n\nThe standard error of the proportion of D in a sample of size \\(n\\) is\n\\[s.e. = \\sqrt{\\frac{\\pi (1-\\pi)}{n}}\\] The standard error is estimated by\n\\[\\hat{s.e.} = \\sqrt{\\frac{p (1-p)}{n}}\\] where \\(\\pi\\) is replaced by \\(p\\)\n\n\n\n\n\n\nexplain\n\n\n\nThe normal approximation, how is it applied in computation of CI\ncheck the histogram as n gets larger\n\n\nWhen the sample size \\(n\\) increases, the binomial distribution can be approximated by a normal distribution. This is useful for computing confidence intervals, and carrying out hypothesis tests.\nThe approximation is valid when \\(n\\pi\\) and \\(n(1-\\pi)\\) is greater or equal to 10.\nThe confidence interval for the population probability, \\(\\pi\\) is\n\\[\\text{CI} = (p - z \\times \\sqrt{\\frac{p(1-p)}{n}}, \\quad p + z \\times \\sqrt{\\frac{p(1-p)}{n}})\\] where \\(z\\) is the \\(1-\\frac{\\alpha}{2}\\) quantile of a standard normal distribution.\nFor 95% confidence interval, \\(\\alpha = 1-0.95 = 0.05\\), so \\(1-\\frac{\\alpha}{2} = 0.975\\) and \\(z = 1.96\\).\n\n\n\n\n\n\nExample\n\n\n\nSmoking habits example"
  },
  {
    "objectID": "summary.html#equations",
    "href": "summary.html#equations",
    "title": "12  Summary",
    "section": "12.1 equations",
    "text": "12.1 equations\n\\[P(E) = \\beta\\]\n\\[CI = \\bigg\\{exp   \\bigg\\}\\]"
  },
  {
    "objectID": "summary.html#tables",
    "href": "summary.html#tables",
    "title": "12  Summary",
    "section": "12.2 tables",
    "text": "12.2 tables\n\n\n\nCol1\nCol2\nCol3\n\n\n\n\nA\nB\nC\n\n\nE\nF\nG\n\n\nA\nG\nG"
  },
  {
    "objectID": "summary.html#colored-text",
    "href": "summary.html#colored-text",
    "title": "12  Summary",
    "section": "12.3 colored text",
    "text": "12.3 colored text\ntext\n\n\n\n\n\n\nexplain\n\n\n\nmore about"
  },
  {
    "objectID": "section2_proportions.html#test-for-one-proportion",
    "href": "section2_proportions.html#test-for-one-proportion",
    "title": "7  Analysis of proportions",
    "section": "7.2 Test for one proportion",
    "text": "7.2 Test for one proportion\n\n\n\n\n\n\nExplain\n\n\n\nname of the test, interpretation of the test statistic and p-value\n\n\nPerform a z-test using the approximating normal distribution, to test the null hypothesis that the population proportion equals a pre-specified value, \\(\\pi_0\\):\n\\[H_0: \\pi = \\pi_0, \\quad H_a:\\pi \\neq\\pi_0\\]\nProviding that both \\(n\\pi_0\\) and \\(n(1-\\pi_0)\\) are greater than 10, the test statistic\n\\[z = \\frac{p-\\pi_0}{\\sqrt{\\pi_0(1-\\pi_0)/n}}\\] is standard normally distributed.\nFrom the test statistic we derive a P-value, which is the probability that \\(\\pi = \\pi_0\\), or more extreme !!!refine this !!!\n\n\n\n\n\n\nExample\n\n\n\nSmoking example"
  },
  {
    "objectID": "section2_proportions.html#comparing-two-proportions",
    "href": "section2_proportions.html#comparing-two-proportions",
    "title": "7  Analysis of proportions",
    "section": "7.3 Comparing two proportions",
    "text": "7.3 Comparing two proportions\nWant to compare two groups with respect to the occurrence of a binary outcome, e.g. getting a certain disease, or whether a drug is effective:\n\nGroup 1: individuals exposed to a risk factor, or treatment\nGroup 0: individuals unexposed to a risk factor, or control/placebo\n\nThere are three common measures for comparing the outcomes between the two groups:\n\nRisk ratio, or relative risk\nOdds ratio\nRisk difference (not very used in practice)\n\nFor each measure we can compute a confidence interval, and carry out a hypothesis test.\n\n7.3.1 2 \\(\\times\\) 2 contingency table\nIndividuals are classified according to their exposure and outcome categories. Cross tabulation is used to display the data in a \\(2 \\times 2\\) contingency table.\n\n\n\n\n\n\nExplain\n\n\n\nshow table\n\n\n\n\n\n\n\n\nExample\n\n\n\nInfluenza vaccine example\n\n\n\n\n7.3.2 Risk difference\n\n\n\n\n\n\nExplain\n\n\n\nprobably wait until the end"
  },
  {
    "objectID": "section2_proportions.html#relative-risk",
    "href": "section2_proportions.html#relative-risk",
    "title": "7  Analysis of proportions",
    "section": "7.4 Relative risk",
    "text": "7.4 Relative risk\nFor two population proportions \\(\\pi_1, \\pi_0\\), the relative risk, or risk ratio RR is the ratio of the two:\n\\[RR = \\pi_1/\\pi_0\\]\nIt is estimated by\n\\[\\hat{RR} = \\frac{p_1}{p_0} = \\frac{d_1/n_1}{d_0/n_0}\\] where \\(p_1, p_0\\) are the sample proportions in the exposed and unexposed groups.\nProperties of the relative risk\n\nRR = 1: the risks (of the outcome) are the same in the two groups\nRR > 1: the risk is higher among those exposed to the risk factor\nRR < 1: the risk is lower among those exposed to the risk factor\nThe further RR is from 1, the stronger the association between exposure and outcome\n\n\n7.4.1 Confidence interval for the relative risk\nThe 95% confidence interval for the relative risk is\n\\[CI = \\bigg(exp \\bigg\\{ log \\hat{RR} \\pm 1.96 \\times \\text{s.e.} (log \\hat{RR})\\bigg\\} \\bigg)\\]\nwhere the estimated standard error of the natural logarithm of the estimated RR is\n\\[\\hat{\\text{s.e.}} (log \\hat{RR}) = \\sqrt{\\frac{1}{d_1} - \\frac{1}{n_1}+ \\frac{1}{d_0} - \\frac{1}{n_0}}\\]\n\n\n\n\n\n\nExample\n\n\n\nAssociation between smoking and lung cancer"
  },
  {
    "objectID": "section2_proportions.html#odds-and-odds-ratio",
    "href": "section2_proportions.html#odds-and-odds-ratio",
    "title": "7  Analysis of proportions",
    "section": "7.5 Odds and odds ratio",
    "text": "7.5 Odds and odds ratio\nThe odds of an outcome D (disease) is defined as\n\\[\\text{Odds} = \\frac{P(\\text{D happens})}{P(\\text{D does not happen})} = \\frac{P(D)}{1-P(D)}\\] The odds is estimated by\n\\[\\hat{\\text{Odds}} = \\frac{p}{1-p} = \\frac{d/n}{1-d/n} = \\frac{d/n}{h/n} = \\frac{d}{h}\\] which is the number of individuals who experience the outcome (e.g. disease) divided by the number of those who do not (healthy).\n\n7.5.1 Odds ratio\nThe odds ratio OR is the ratio between the odds in the exposed group, and the odds in the unexposed group. It is estimated by\n\\[\\hat{OR} = \\frac{d_1/h_1}{d_0/h_0} = \\frac{d_1 \\times h_0}{d_0 \\times h_1}\\] It is the cross-produce ratio of the \\(2 \\times 2\\) table.\nOR is one of the most common effect measures in medical statistics, even though it is less intuitive than RR. OR = 1 occurs when the odds (hence proportions) are the same in two groups.\nThe OR is always further away from 1 compared to RR. For rare outcomes, OR is approximately equal to the RR.\n\n\n\n\n\n\nExample\n\n\n\nNausea vs new drug, compute relative risk and odds ratio\n\n\n\n\n\n\n\n\nExplain\n\n\n\nWhen to choose what measure\nthe comparison\n\n\n\n\n7.5.2 Confidence interval for OR\nThe 95% confidence interval for the odds ratio OR is\n\\[CI = \\bigg(exp \\bigg\\{ log \\hat{OR} \\pm 1.96 \\times \\text{s.e.} (log \\hat{OR})\\bigg\\} \\bigg)\\]\nwhere the estimated standard error of the natural logarithm of the estimated odds ratio is\n\\[\\hat{\\text{s.e.}} (log \\hat{OR}) = \\sqrt{\\frac{1}{d_1} + \\frac{1}{h_1}+ \\frac{1}{d_0} + \\frac{1}{h_0}}\\] This is also known as Woolf’s formula\n\n\n\n\n\n\nExample\n\n\n\nAsthma among women and men"
  },
  {
    "objectID": "section2_tables.html#chi-squared-test",
    "href": "section2_tables.html#chi-squared-test",
    "title": "8  Tables, chi-sq, exact tests",
    "section": "8.1 Chi-squared test",
    "text": "8.1 Chi-squared test\nThe test statistic for Chi-squared test for a \\(2 \\times 2\\) table is\n\\[\\chi^2 = \\sum_i \\frac{(O_i - E_i)^2}{E_i}, \\quad \\text{d.f.} = 1\\] where \\(O_i, E_i\\) is the observed and expected values in the \\(i_{th}\\) cell.\nFor a \\(2 \\times 2\\) table, the test statistic is chi-squared distributed with 1 degree of freedom under the null hypothesis, of no association between the two variables.\nThis is equivalent to the z-test for the difference between two proportions.\n\n\n\n\n\n\nExplain\n\n\n\nExplain the logic of the test, explain what is degree of freedom\nand the relevance with z-test for proportions\nand how it differs from RR, OR\n\n\n\n\n\n\n\n\nExample\n\n\n\nInfluenza trial example\n\n\nA second formulation:\n\\[\\chi^2 = \\frac{n \\times (d_1 h_0 - d_0 h_1)^2}{d\\times h \\times n_1 n_0}, \\quad \\text{d.f.} = 1\\]\n\n\n\n\n\n\nExample\n\n\n\nInfluenza trial example"
  },
  {
    "objectID": "section2_tables.html#chi-squared-test-for-2-times-2-tables",
    "href": "section2_tables.html#chi-squared-test-for-2-times-2-tables",
    "title": "8  Analysis of contingency tables",
    "section": "8.1 Chi-squared test for \\(2 \\times 2\\) tables",
    "text": "8.1 Chi-squared test for \\(2 \\times 2\\) tables\nThe test statistic for Chi-squared test for a \\(2 \\times 2\\) table is\n\\[\\chi^2 = \\sum_i \\frac{(O_i - E_i)^2}{E_i}, \\quad \\text{d.f.} = 1\\] where \\(O_i, E_i\\) is the observed and expected values in the \\(i_{th}\\) cell.\nFor a \\(2 \\times 2\\) table, the test statistic is chi-squared distributed with 1 degree of freedom under the null hypothesis, of no association between the two variables.\nThis is equivalent to the z-test for the difference between two proportions.\n\n\n\n\n\n\nExplain\n\n\n\nExplain the logic of the test, explain what is degree of freedom\nand the relevance with z-test for proportions\nand how it differs from RR, OR\n\n\n\n\n\n\n\n\nExample\n\n\n\nInfluenza trial example\n\n\nA second formulation:\n\\[\\chi^2 = \\frac{n \\times (d_1 h_0 - d_0 h_1)^2}{d\\times h \\times n_1 n_0}, \\quad \\text{d.f.} = 1\\]\n\n\n\n\n\n\nExample\n\n\n\nInfluenza trial example"
  },
  {
    "objectID": "section2_tables.html#fishers-exact-test",
    "href": "section2_tables.html#fishers-exact-test",
    "title": "8  Analysis of contingency tables",
    "section": "8.2 Fisher’s Exact Test",
    "text": "8.2 Fisher’s Exact Test\nWhen the numbers in the \\(2 \\times 2\\) table are very small, we need an exact test to compare the proportions.\nThis is based on calculating the exact probabilities of the observed table, and of more extreme tables with the same row and column totals, using the following formula\n\\[\\text{Exact probability} = \\frac{d! \\times h! \\times n_1 ! \\times n_0!}{n!\\times d_1! \\times d_0! \\times h_1! \\times h_0!}\\]\nwith the standard notation for \\(2 \\times 2\\) table.\n\n\n\n\n\n\nExplain\n\n\n\nLogic of the test\n\n\n\n8.2.1 P-value for the exact test\nApproach I:\nP-val = probability of obbserved table + probability of less probable tabless\nApproach II:\nP-val = \\(2 \\times(\\) probability of observed table + probability of more extreme tables in the same direction\\()\\)\n\n\n\n\n\n\nExample\n\n\n\nBleeding example"
  },
  {
    "objectID": "section2_tables.html#chi-squared-test-for-larger-tables",
    "href": "section2_tables.html#chi-squared-test-for-larger-tables",
    "title": "8  Analysis of contingency tables",
    "section": "8.3 Chi-squared test for larger tables",
    "text": "8.3 Chi-squared test for larger tables\nChi-squared test can also be applied to larger tables, which can be denoted as \\(r \\times c\\) tables. \\(r\\) and \\(c\\) are the number of rows and columns in the table.\nTest statistic:\n\\[\\chi^2 = \\sum_i \\frac{(O_i - E_i)^2}{E_i}, \\quad \\text{d.f.} = (r-1)\\times (c-1)\\] which is chi-squared distributed with \\((r-1)\\times (c-1)\\) degrees of freedom. For \\(2 \\times 2\\) tables, this reduces to 1 df.\nTo compute the expected number,\n\\[E = \\frac{\\text{column total} \\times \\text{row total}}{\\text{overall total}}\\]\nThe approximation of chi-squared test is valid when\n\nless than 20% of the expected numbers are under 5, and\nnone of the expected numbers is less than 1.\n\nSometimes the restrictions can be overcome by combining rows (or columns) with low expected numbers, providing that these combinations make biological sense."
  },
  {
    "objectID": "section2_tables.html#which-test-to-choose",
    "href": "section2_tables.html#which-test-to-choose",
    "title": "8  Analysis of contingency tables",
    "section": "8.4 Which test to choose",
    "text": "8.4 Which test to choose\nThe two tests have different assumptions.\nChi-squared test is valid when\n\nthe overall total is more than 40, regardless of the expected values; or,\nthe overall total is between 20 and 40, provided all the expected values are at least 5\n\nExact test is recommended when\n\nthe overall total is less than 20; or,\nthe overall total is betweenn 20 and 40, and the smallest of the four expected numbers is less than 5\n\nThe exact test can be very conservative - giving high p-values, and low power (i.e. fail to detect true effect)\n\n\n\n\n\n\nExplain\n\n\n\nRead about the choice of test: Lydersen, Fagerland, Laake, Recommended tests for association in 2x2 tables, Statistics in Medicine, 2009: Fisher’s exact should never be used without the mid-P correction"
  },
  {
    "objectID": "section9_stata.html#get-started",
    "href": "section9_stata.html#get-started",
    "title": "11  Introduction to STATA",
    "section": "11.1 Get started",
    "text": "11.1 Get started\nBasic components of the user interface:\n\ncommand window: type in command to tell stata what to do\ndo-file editor: write and execute do-file scripts\ndata editor: makes it easy to edit and inspect data\n\n\nWhen you execute a command, write the command in the command window, then hit Enter.\nFor example, you can execute the command use to load data, and use browse to open up the data editor.\n\nData editor looks like this:\n\n\n11.1.1 File system: where things are\nIt is recommended to have different folders to stay organized. For example, you can have\n\none folder for your scripts (such as do-file),\none folder for data (for examplem, csv, dta, xlsx),\none folder for documentation.\n\n\n\n11.1.2 Get help\nIn the command, type help *command_name* to open up the documentation:\n\nAlternatively, use Google."
  },
  {
    "objectID": "section9_stata.html#data-import",
    "href": "section9_stata.html#data-import",
    "title": "11  Introduction to STATA",
    "section": "11.2 Data import",
    "text": "11.2 Data import"
  },
  {
    "objectID": "section9_stata.html#data-entry-and-import",
    "href": "section9_stata.html#data-entry-and-import",
    "title": "11  Introduction to STATA",
    "section": "11.2 Data entry and import",
    "text": "11.2 Data entry and import\nData format: data.dta\n\n11.2.1 Enter data manually\n\n\n11.2.2 Import data\nInspect the data: either browse, or click the icon\nInspect a few variables: browse age eth to select age and eth variables\ncodebook age to inspect age variable"
  },
  {
    "objectID": "section9_stata.html#data-import-entry-and-manipulation",
    "href": "section9_stata.html#data-import-entry-and-manipulation",
    "title": "11  Introduction to STATA",
    "section": "11.2 Data import, entry and manipulation",
    "text": "11.2 Data import, entry and manipulation\nData format: data.dta\n\n11.2.1 Import data\nInspect the data: either browse, or click the icon\nInspect a few variables: browse age eth to select age and eth variables\ncodebook age to inspect age variable\nUse clear to remove data object\n\n\n11.2.2 Enter data manually\nUse data editor\n\n\n11.2.3 Basic manipulation\nrename var1 name"
  },
  {
    "objectID": "section9_stata.html#data-entry-import-and-manipulation",
    "href": "section9_stata.html#data-entry-import-and-manipulation",
    "title": "11  Introduction to STATA",
    "section": "11.2 Data entry, import and manipulation",
    "text": "11.2 Data entry, import and manipulation\nData format: data.dta\n\n11.2.1 Enter data manually\nOpen the data editor, and type in data. This is equivalent to the commands in the command window, which you can see in the Results panel.\n\nSave your data by clicking Save on the data editor. This is equivalent to the following command:\nsave \"data/path/demo_data.dta\"\n\n\n11.2.2 Import data\nLoad (import) data by the command use, then specify the path of the data file.\nWrite browse to view the data, or alternatively, click the data editor icon.\nuse \"data/path/demo_data.dta\"\n\n\n\n11.2.3 Basic manipulation\nChange variable names with rename\nrename var1 name\nrename var2 gender\nrename var3 age\nAdd labels to variables, or change their properties with label command.\nIt is worth noting that stata recognize double quote \"\". Single quote will produce errors."
  },
  {
    "objectID": "section9_stata.html#use-do-files",
    "href": "section9_stata.html#use-do-files",
    "title": "11  Introduction to STATA",
    "section": "11.3 Use do files",
    "text": "11.3 Use do files\ndo files are useful to save and document your analysis for the future. An example looks like this:\n\nTo run (execute) a do file, write do *filename.do* in the command; alternatively, hit Do icon\ndo filename.do\nYou might need to use clear to remove data object before creating a new data."
  }
]